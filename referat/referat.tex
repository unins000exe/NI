\documentclass[bachelor, och, referat]{SCWorks}
% параметр - тип обучения - одно из значений:
%    spec     - специальность
%    bachelor - бакалавриат (по умолчанию)
%    master   - магистратура
% параметр - форма обучения - одно из значений:
%    och   - очное (по умолчанию)
%    zaoch - заочное
% параметр - тип работы - одно из значений:
%    referat    - реферат
%    coursework - курсовая работа (по умолчанию)
%    diploma    - дипломная работа
%    pract      - отчет по практике
% параметр - включение шрифта
%    times    - включение шрифта Times New Roman (если установлен)
%               по умолчанию выключен
\usepackage{subfigure}
\usepackage{tikz,pgfplots}
\pgfplotsset{compat=1.5}
\usepackage{float}

%\usepackage{titlesec}
\setcounter{secnumdepth}{4}
%\titleformat{\paragraph}
%{\normalfont\normalsize}{\theparagraph}{1em}{}
%\titlespacing*{\paragraph}
%{35.5pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

\titleformat{\paragraph}[block]
{\hspace{1.25cm}\normalfont}
{\theparagraph}{1ex}{}
\titlespacing{\paragraph}
{0cm}{2ex plus 1ex minus .2ex}{.4ex plus.2ex}

% --------------------------------------------------------------------------%
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\usepackage{tempora}

\usepackage[sort,compress]{cite}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{listingsutf8}
\usepackage{longtable}
\usepackage{array}
\usepackage[english,russian]{babel}

\usepackage[colorlinks=true, linkcolor=black]{hyperref}
\usepackage{url}

\usepackage{underscore}
\usepackage{setspace}
\usepackage{indentfirst} 
\usepackage{mathtools}
\usepackage{amsfonts}
\usepackage{enumitem}
\usepackage{tikz}

\usepackage{minted}
\setminted[python3]{style=bw, linenos, breaklines=true, fontsize=\footnotesize}

\newcommand{\eqdef}{\stackrel {\rm def}{=}}
\newcommand{\specialcell}[2][c]{%
\begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

\renewcommand\theFancyVerbLine{\small\arabic{FancyVerbLine}}

\newtheorem{lem}{Лемма}

\begin{document}

% Кафедра (в родительном падеже)
\chair{теоретических основ компьютерной безопасности и криптографии}

% Тема работы
\title{Нейронные сети. Обучение без учителя и кластеризация данных}

% Курс
\course{5}

% Группа
\group{531}

% Факультет (в родительном падеже) (по умолчанию "факультета КНиИТ")
\department{факультета КНиИТ}

% Специальность/направление код - наименование
%\napravlenie{09.03.04 "--- Программная инженерия}
%\napravlenie{010500 "--- Математическое обеспечение и администрирование информационных систем}
%\napravlenie{230100 "--- Информатика и вычислительная техника}
%\napravlenie{231000 "--- Программная инженерия}
\napravlenie{10.05.01 "--- Компьютерная безопасность}

% Для студентки. Для работы студента следующая команда не нужна.
% \studenttitle{Студентки}

% Фамилия, имя, отчество в родительном падеже
\author{Стаина Романа Игоревича}

% Заведующий кафедрой
\chtitle{} % степень, звание
\chname{}

%Научный руководитель (для реферата преподаватель проверяющий работу)
\satitle{доцент} %должность, степень, звание
\saname{И.~И.~Слеповичев}

% Руководитель практики от организации (только для практики,
% для остальных типов работ не используется)
% \patitle{к.ф.-м.н.}
% \paname{С.~В.~Миронов}

% Семестр (только для практики, для остальных
% типов работ не используется)
%\term{8}

% Наименование практики (только для практики, для остальных
% типов работ не используется)
%\practtype{преддипломная}

% Продолжительность практики (количество недель) (только для практики,
% для остальных типов работ не используется)
%\duration{4}

% Даты начала и окончания практики (только для практики, для остальных
% типов работ не используется)
%\practStart{30.04.2019}
%\practFinish{27.05.2019}

% Год выполнения отчета
\date{2023}

\maketitle

% Включение нумерации рисунков, формул и таблиц по разделам
% (по умолчанию - нумерация сквозная)
% (допускается оба вида нумерации)
% \secNumbering

%-------------------------------------------------------------------------------------------

% \begin{minted}[fontsize=\small]{MySQL}
% \end{minted}

% \begin{figure}[H]
%     \centering
%     \includegraphics[width=0.999\textwidth]{img/}
%     \caption{}
%     \label{easy_hack}
% \end{figure}

\tableofcontents

\intro 
Алгоритмы обучения с учителем нейронных сетей подразумевают наличие некоего внешнего звена, 
предоставляющего сети, кроме входных, также и целевые выходные образы. 
Для их успешного функционирования необходимо наличие экспертов, 
создающих на предварительном этапе для каждого входного образа эталонный выходной. 
Обучения без учителя, наоборот, не требует разметки данных. 
Система старается сама найти в них общие признаки и связи.

Нейронные сети, обученные без учителя, чаще всего используются
для задачи кластеризации данных, где выборка объектов разбивается на непересекающиеся подмножества, 
называемые кластерами, так, чтобы каждый кластер состоял из схожих объектов, 
а объекты разных кластеров существенно отличались. 

Кластеризация обычно применяется для следующих целей:
\begin{itemize}
    \item Визуализация данных (наглядное представление многомерных данных).
    \item Сегментация рынка (определение типов клиентов).
    \item Рекомендательные системы (на основе кластеризации пользователей можно предлагать им товары или услуги, которые могут их заинтересовать).
    \item Объединение близких точек на карте (может использоваться для сжатия изображений).
    \item Обнаружение выбросов (помогает выявить аномальные значения в наборе данных и устранить их).
\end{itemize}

\section{Обучение без учителя}
https://tproger.ru/articles/kak-rabotaet-obuchenie-bez-uchitelya

\subsection{Сигнальный метод обучения Хебба}
https://nauchniestati.ru/spravka/obuchenie-hebba/

https://masters.donntu.ru/2006/kita/chvala/library/N3.pdf

Сигнальный метод обучения Хебба заключается в изменении весов по следующему
правилу:
\[ w_{ij}(t) = w_{ij}(t - 1) + \alpha \cdot y_i^{(n - 1)} \cdot y_j^{(n)} \; \; \; (1)\]
где $y_i^{(n - 1)}$ -- выходное значение нейрона $i$ слоя 
$(n - 1)$, $y_j^{(n)}$ -- выходное значение нейрона $j$ слоя $n$;
$w_{ij}(t)$ и $w_{ij}(t - 1)$ -- весовой коэффициент синапса, 
соединяющего эти нейроны, на итерациях $t$ и $t - 1$ соответственно,
$\alpha$ -- коэффициент скорости обучения. Здесь и далее, для
общности, под $n$ подразумевается произвольный слой сети. 
При обучении по данному методу усиливаются связи между возбужденными нейронами. 

Существует также и дифференциальный метод обучения Хебба.
\[ w_{ij}(t) = w_{ij}(t - 1) + \alpha \cdot [y_i^{(n-1)}(t) - 
y_i^{(n - 1)}(t - 1)] \cdot [y_j^{(n)}(t) - y_j^{(n)}(t - 1)] \; \; \; (2)\]
Здесь $y_i^{(n-1)}(t)$ и $y_i^{(n - 1)}(t - 1)$ -- выходное
значение нейрона $i$ слоя $n - 1$ соответственно на итерациях
$t$ и $t - 1$; $y_j^{(n)}(t)$ и $y_j^{(n)}(t - 1)$ -- то же самое
для нейрона $j$ слоя $n$. Как видно из формулы $(2)$,
сильнее всего обучаются синапсы, соединяющие те нейроны,
выходы которых наиболее динамично изменились в сторону увеличения.

\subsubsection{Алгоритм обучения с примнением метода Хебба}
\begin{enumerate}
    \item На стадии инициализации всем весовым коэффициентам присваиваются небольшие
    случайные значения.
    \item На входы сети подается входной образ, и сигналы возбуждения распространяются по
    всем слоям согласно принципам классических прямопоточных сетей, 
    то есть для каждого нейрона рассчитывается взвешенная сумма его входов, 
    к которой затем применяется функция активации нейрона, в результате
    чего получается выходное значение $y_i^{(n)}, i = 0, \dots, M_i - 1$,
    где $M_i$ -- число нейронов в слое $i; n = 0, \dots, N - 1$, а $N$
    -- число слоёв в сети.
    \item На основании полученных выходных значений нейронов по формуле $(1)$ или (2)
    производится изменение весовых коэффициентов. 
    \item Цикл с шага 2, пока выходные значения сети не застабилизируются с заданной точностью.
\end{enumerate}

Применение этого способа определения завершения обучения, отличного от
использовавшегося для сети обратного распространения, обусловлено тем, что подстраиваемые
значения синапсов фактически не ограничены. 

На втором шаге цикла попеременно предъявляются все образы из входного набора.
Следует отметить, что вид откликов на каждый класс входных образов не известен
заранее и будет представлять собой произвольное сочетание состояний нейронов выходного
слоя, обусловленное случайным распределением весов на стадии инициализации. Вместе с тем,
сеть способна обобщать схожие образы, относя их к одному классу. Тестирование обученной
сети позволяет определить топологию классов в выходном слое. Для приведения откликов
обученной сети к удобному представлению можно дополнить сеть одним слоем, который,
например, по алгоритму обучения однослойного перцептрона необходимо заставить отображать
выходные реакции сети в требуемые образы.

\section{Кластеризация данных}
Кластеризация

https://www.mathnet.ru/links/9a5e61bf9982a36450ce37e839a997ca/at11199.pdf

\subsection{Метод k-средних}
https://www.elibrary.ru/download/elibrary_30574809_10614907.pdf

https://ru.wikipedia.org/wiki/Метод_k-средних

\conclusion
Таким образом, обучение без учителя -- это процесс обучения модели на основе неразмеченных данных, где нет известных меток или целевых переменных. В этом случае модель ищет скрытые структуры и закономерности в данных, чтобы создать представление или кластеризацию данных.

Преимуществами данного метода обучения являются:
\begin{enumerate}
    \item Извлечение скрытых структур. Обучение без учителя позволяет модели извлекать скрытые структуры и паттерны из данных, что может быть полезно для обнаружения новых знаний и понимания данных.
    \item Работа с большими объемами данных. Обучение без учителя может быть эффективным при работе с большими объемами данных, поскольку нет необходимости размечать каждый пример.
    \item Автоматическое обучение. Обучение без учителя позволяет модели самостоятельно находить паттерны и структуры в данных, без необходимости вручную определять правильные ответы.
\end{enumerate}

Недостатки:
\begin{enumerate}
    \item Неопределенность результатов. Результаты обучения без учителя могут быть менее интерпретируемыми и требуют дополнительного анализа и проверки.
    \item Трудность оценки. Оценка качества модели в обучении без учителя может быть сложной, поскольку нет явных правильных ответов для сравнения.
    \item Необходимость предварительной обработки данных. В обучении без учителя может потребоваться предварительная обработка данных для удаления шума или выбросов, что может быть трудоемким процессом.
\end{enumerate}

\end{document}